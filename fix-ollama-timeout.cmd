@echo off
echo ü§ñ CORRECTION TIMEOUT OLLAMA
echo.

echo üìã 1. V√©rification du statut OLLAMA...
docker exec ollama ollama list

echo.
echo üìã 2. V√©rification des logs OLLAMA...
docker logs ollama --tail 15

echo.
echo üîß 3. Red√©marrage d'OLLAMA...
docker-compose restart ollama

echo.
echo ‚è≥ 4. Attente du chargement du mod√®le (60 secondes)...
timeout /t 60

echo.
echo üß™ 5. Test du mod√®le...
echo Test simple du mod√®le OLLAMA...
docker exec ollama ollama run llama3:8b "Hello, are you working?" || echo "‚ùå Mod√®le non r√©pondant"

echo.
echo üìã 6. V√©rification des variables d'environnement API...
docker exec api env | findstr OLLAMA || echo "‚ùå Variables OLLAMA manquantes"

echo.
echo üîß 7. Solutions sp√©cifiques au timeout:

echo.
echo A. Si le mod√®le ne r√©pond pas:
echo    - Le mod√®le est peut-√™tre trop volumineux pour votre RAM
echo    - Essayez un mod√®le plus petit: ollama pull llama3.2:1b
echo.
echo B. Si l'API timeout (240s):
echo    - Le mod√®le met trop de temps √† g√©n√©rer la r√©ponse
echo    - R√©duisez la longueur de vos questions
echo    - V√©rifiez la charge CPU/RAM
echo.
echo C. Configuration recommand√©e:
echo    - RAM minimum: 8GB pour llama3:8b
echo    - RAM recommand√©e: 16GB
echo    - CPU: 4+ cores
echo.
echo ‚úÖ PROCHAINES √âTAPES:
echo 1. Attendez 2-3 minutes que OLLAMA se stabilise
echo 2. Testez avec une question courte (max 10 mots)
echo 3. Si toujours lent, changez de mod√®le:
echo    docker exec ollama ollama pull llama3.2:1b
echo    Puis modifiez OLLAMA_MODEL_NAME=llama3.2:1b dans .env
echo.
pause 