@echo off
echo ğŸš€ DÃ‰MARRAGE OLLAMA avec votre modÃ¨le existant
echo.

echo ğŸ“‹ ModÃ¨les dÃ©tectÃ©s:
echo - llama3:latest (4.7 GB)
echo - llama3:8b (4.7 GB)
echo.

echo ğŸ”„ DÃ©marrage du serveur OLLAMA...
echo âš ï¸  IMPORTANT: Gardez cette fenÃªtre ouverte !
echo.

echo DÃ©marrage en cours...
start "OLLAMA Server" cmd /k "echo ğŸ”´ SERVEUR OLLAMA ACTIF - NE FERMEZ PAS CETTE FENETRE && ollama serve"

echo â³ Attente de 10 secondes pour initialisation...
timeout /t 10

echo.
echo ğŸ§ª Test de connectivitÃ© OLLAMA...
curl -s http://localhost:11434/api/tags >nul 2>&1
if %ERRORLEVEL% EQU 0 (
    echo âœ… OLLAMA API accessible !
) else (
    echo âš ï¸ OLLAMA pas encore prÃªt, attente supplÃ©mentaire...
    timeout /t 10
    curl -s http://localhost:11434/api/tags >nul 2>&1
    if %ERRORLEVEL% EQU 0 (
        echo âœ… OLLAMA API maintenant accessible !
    ) else (
        echo âŒ ProblÃ¨me de dÃ©marrage OLLAMA
    )
)

echo.
echo ğŸ§ª Test rapide du modÃ¨le llama3:8b...
echo (Cela peut prendre 30-60 secondes la premiÃ¨re fois)
ollama run llama3:8b "Bonjour, rÃ©ponds briÃ¨vement" --timeout 60

echo.
echo ğŸ”§ RedÃ©marrage de votre API pour appliquer les changements...
docker-compose restart fastapi_api

echo.
echo âœ… CONFIGURATION TERMINÃ‰E !
echo.
echo ğŸ¯ Votre configuration:
echo - OLLAMA local avec llama3:8b (parfait pour votre API)
echo - API: host.docker.internal:11434 âœ…
echo - ModÃ¨le: llama3:8b âœ…
echo.
echo ğŸ“ TESTS Ã€ EFFECTUER:
echo 1. Allez sur votre chatbot: http://localhost
echo 2. Connectez-vous
echo 3. Posez une question: "Bonjour"
echo 4. La rÃ©ponse devrait arriver en 10-30 secondes (plus de timeout!)
echo.
echo âš ï¸ RAPPEL: Ne fermez pas la fenÃªtre "OLLAMA Server" qui s'est ouverte
echo.
pause 