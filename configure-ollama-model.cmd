@echo off
echo ğŸ”§ CONFIGURATION MODÃˆLE OLLAMA LÃ‰GER
echo.

echo ğŸ“¥ TÃ©lÃ©chargement du modÃ¨le llama3.2:1b (plus rapide)...
ollama pull llama3.2:1b

echo.
echo ğŸ§ª Test du modÃ¨le...
ollama run llama3.2:1b "Bonjour, rÃ©ponds en franÃ§ais s'il te plaÃ®t"

echo.
echo âš™ï¸ Pour utiliser ce modÃ¨le par dÃ©faut dans votre API:
echo.
echo 1. CrÃ©ez un fichier .env dans votre projet
echo 2. Ajoutez cette ligne: OLLAMA_MODEL_NAME=llama3.2:1b
echo 3. Ou modifiez votre configuration existante
echo.
echo ğŸ“ Configuration actuelle dans docker-compose.yml:
echo    OLLAMA_MODEL_NAME: ${OLLAMA_MODEL_NAME:-llama3:8b}
echo.
echo ğŸ’¡ Avec .env, cela deviendra: llama3.2:1b
echo.
echo âœ… AVANTAGES du modÃ¨le llama3.2:1b:
echo - Plus rapide (rÃ©ponse en 2-10 secondes au lieu de 30-240s)
echo - Moins de RAM nÃ©cessaire (2GB au lieu de 8GB)
echo - Parfait pour un chatbot rapide
echo.
pause 