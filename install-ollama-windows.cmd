@echo off
echo ü§ñ INSTALLATION OLLAMA POUR WINDOWS
echo.

echo üì• 1. T√©l√©chargement d'OLLAMA...
echo Allez sur: https://ollama.com/download/windows
echo.
echo OU utilisez PowerShell pour t√©l√©charger automatiquement:
echo.
powershell -Command "& {Invoke-WebRequest -Uri 'https://ollama.com/download/OllamaSetup.exe' -OutFile 'OllamaSetup.exe'}"

echo.
echo üì¶ 2. Installation manuelle...
echo Double-cliquez sur OllamaSetup.exe pour installer
echo.
pause

echo.
echo üöÄ 3. D√©marrage du service OLLAMA...
echo D√©marrage d'OLLAMA en arri√®re-plan...
start "" ollama serve

echo.
echo ‚è≥ 4. Attente de 10 secondes...
timeout /t 10

echo.
echo üì• 5. T√©l√©chargement du mod√®le (cela peut prendre 10-30 minutes)...
echo T√©l√©chargement de llama3.2:1b (mod√®le l√©ger - 1.3GB)...
ollama pull llama3.2:1b

echo.
echo üß™ 6. Test du mod√®le...
ollama run llama3.2:1b "Hello, can you respond in French?"

echo.
echo ‚úÖ 7. Configuration termin√©e!
echo.
echo üéØ PROCHAINES √âTAPES:
echo 1. Modifiez votre fichier .env:
echo    OLLAMA_MODEL_NAME=llama3.2:1b
echo.
echo 2. Red√©marrez votre API:
echo    docker-compose restart fastapi_api
echo.
echo 3. Testez votre chatbot avec une question courte
echo.
echo üìù NOTES:
echo - OLLAMA fonctionne maintenant sur localhost:11434
echo - Le mod√®le llama3.2:1b est plus rapide que llama3:8b
echo - Votre API peut maintenant communiquer avec OLLAMA
echo.
pause 